{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T18:08:53.684862Z",
     "start_time": "2023-11-25T18:08:53.483907Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "from scipy.spatial import distance\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 8})\n",
    "plt.rcParams[\"svg.fonttype\"] = \"none\"\n",
    "import seaborn as sns\n",
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "\n",
    "sys.path.extend([r'/home/lindenmp/research_projects/snaplab_tools'])\n",
    "sys.path.extend([r'/home/lindenmp/research_projects/nctpy/src'])\n",
    "\n",
    "# import nctpy functions\n",
    "from snaplab_tools.plotting.plotting import categorical_kde_plot, reg_plot, brain_scatter_plot, null_plot, surface_plot\n",
    "from snaplab_tools.plotting.utils import get_my_colors, get_p_val_string\n",
    "from nctpy.utils import matrix_normalization\n",
    "from snaplab_tools.utils import get_schaefer_system_mask, get_null_p, get_fdr_p\n",
    "from snaplab_tools.plotting.utils import roi_to_vtx\n",
    "\n",
    "from brainsmash.mapgen.base import Base\n",
    "from src.utils import get_adj_weights\n",
    "from snaplab_tools.utils import normalize_x\n",
    "\n",
    "from snaplab_tools.derivs import compute_acf\n",
    "from nilearn.glm import first_level\n",
    "from nilearn import plotting\n",
    "from nilearn.glm.contrasts import compute_contrast\n",
    "from scipy import signal\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_deflections(rest_timescales, task_timescales, nuisance_regression=False):\n",
    "    rest_timescales = rest_timescales.reshape(-1, 1)\n",
    "    task_timescales = task_timescales.reshape(-1, 1)\n",
    "\n",
    "    int_deflections = task_timescales - rest_timescales\n",
    "    \n",
    "    if nuisance_regression:\n",
    "        deflections_mean = np.mean(int_deflections)\n",
    "        \n",
    "        X = np.concatenate((rest_timescales, np.ones((rest_timescales.shape[0], 1))), axis=1)\n",
    "        beta = np.dot(np.linalg.pinv(X), int_deflections)\n",
    "        predicted = np.dot(X, beta)\n",
    "        int_deflections = int_deflections - predicted\n",
    "        # print(int_deflections.shape, predicted.shape)\n",
    "        # int_deflections += deflections_mean\n",
    "    \n",
    "    return int_deflections[:, 0]\n",
    "\n",
    "def nuis_reg(X, y, use_sklearn=False):\n",
    "    if use_sklearn:\n",
    "        if X.ndim == 1:\n",
    "            X = X[:, np.newaxis]\n",
    "        if y.ndim == 1:\n",
    "            y = y[:, np.newaxis]\n",
    "\n",
    "        regr = LinearRegression()\n",
    "        regr.fit(X, y)\n",
    "        predicted = regr.predict(X)\n",
    "        residuals = y - predicted\n",
    "    else:\n",
    "        beta = np.dot(np.linalg.pinv(X), y)\n",
    "        predicted = np.dot(X, beta)\n",
    "        residuals = y - predicted\n",
    "    return residuals\n",
    "\n",
    "def run_regression(df_behavior, y_var, rest_timescales, task_timescales):\n",
    "    n_nodes = rest_timescales.shape[0]\n",
    "    effect_map = pd.DataFrame(index=np.arange(n_nodes))\n",
    "    pvals = pd.DataFrame(index=np.arange(n_nodes))\n",
    "\n",
    "    for i in tqdm(np.arange(n_nodes)):\n",
    "        df_stats = pd.DataFrame(index=df_behavior.index)\n",
    "        df_stats[y_var] = df_behavior[y_var].copy()\n",
    "        df_stats[y_var].fillna(np.nanmean(df_stats[y_var]), inplace=True)\n",
    "        df_stats[y_var] = sp.stats.zscore(df_stats[y_var])\n",
    "        \n",
    "        df_stats['rest'] = sp.stats.zscore(rest_timescales[i])\n",
    "        df_stats['task'] = sp.stats.zscore(task_timescales[i])\n",
    "        df_stats['age'] = sp.stats.zscore(df_behavior['age'])\n",
    "        df_stats['sex'] = df_behavior['sex']\n",
    "        df_stats = sm.add_constant(df_stats)\n",
    "        # if i == 0:\n",
    "            # print(df_stats.head())\n",
    "        \n",
    "        # Fit the model\n",
    "        results = smf.ols(formula='{0} ~ rest * task + age + C(sex)'.format(y_var), data=df_stats).fit()\n",
    "        # if i == 0:\n",
    "            # print(results.summary())\n",
    "        \n",
    "        for effect in ['rest', 'task', 'rest:task', 'age', 'C(sex)[T.1]']:\n",
    "            try:\n",
    "                effect_map.loc[i, effect] = results.params[effect]\n",
    "                pvals.loc[i, effect] = results.pvalues[effect]\n",
    "            except:\n",
    "                pass\n",
    "        effect_map.loc[i, 'rsquared'] = results.rsquared_adj\n",
    "        \n",
    "    return effect_map, pvals\n",
    "\n",
    "def detect_univariate_outliers(x, k=1.5):\n",
    "    q1, q3 = np.percentile(x, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - (k * iqr)\n",
    "    upper_bound = q3 + (k * iqr)\n",
    "    outlier_labels = ((x < lower_bound) | (x > upper_bound)).astype(int)\n",
    "    \n",
    "    return outlier_labels\n",
    "\n",
    "def compute_r2(x, y):\n",
    "    if x.ndim == 1:\n",
    "        x = x[:, np.newaxis]\n",
    "    if y.ndim == 1:\n",
    "        y = y[:, np.newaxis]\n",
    "\n",
    "    regr = LinearRegression()\n",
    "    regr.fit(x, y)\n",
    "    y_pred = regr.predict(x)\n",
    "    return r2_score(y, y_pred) * 100\n",
    "\n",
    "def winsorize_iqr(vector, k=1.5, inplace=False):\n",
    "    \"\"\"\n",
    "    Winsorizes a vector using the IQR method to handle outliers.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    vector : array-like\n",
    "        Input data to be winsorized (list, numpy array, or pandas Series)\n",
    "    k : float, optional (default=1.5)\n",
    "        Multiplier for IQR to determine outlier thresholds\n",
    "    inplace : bool, optional (default=False)\n",
    "        If True, modifies the input vector in place (only works with mutable input)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    winsorized_vector : numpy array\n",
    "        Winsorized version of the input vector\n",
    "    \"\"\"\n",
    "    # Convert input to numpy array if it isn't already\n",
    "    if not isinstance(vector, np.ndarray):\n",
    "        vector = np.array(vector)\n",
    "    \n",
    "    # Calculate quartiles and IQR\n",
    "    q1 = np.percentile(vector, 25)\n",
    "    q3 = np.percentile(vector, 75)\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    # Calculate lower and upper bounds\n",
    "    lower_bound = q1 - k * iqr\n",
    "    upper_bound = q3 + k * iqr\n",
    "    \n",
    "    # Create a copy unless inplace is True and input is mutable\n",
    "    if inplace and isinstance(vector, np.ndarray):\n",
    "        winsorized_vector = vector\n",
    "    else:\n",
    "        winsorized_vector = vector.copy()\n",
    "    \n",
    "    # Winsorize the values\n",
    "    winsorized_vector[winsorized_vector < lower_bound] = lower_bound\n",
    "    winsorized_vector[winsorized_vector > upper_bound] = upper_bound\n",
    "    \n",
    "    return winsorized_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory where data is stored\n",
    "indir = '/home/lindenmp/research_projects/nct_xr/data/int_deflections'\n",
    "which_data = 'HCP_YA'\n",
    "# which_data = 'HCP_D'\n",
    "outdir = '/home/lindenmp/research_projects/nct_xr/results/int_deflections/{0}'.format(which_data.replace('_', ''))\n",
    "\n",
    "atlas = 'Schaefer4007'\n",
    "if atlas == 'Schaefer4007':\n",
    "    n_parcels = 400\n",
    "    n_nodes = 400\n",
    "elif atlas == 'Schaefer2007':\n",
    "    n_parcels = 200\n",
    "    n_nodes = 200\n",
    "elif atlas == 'Schaefer1007':\n",
    "    n_parcels = 100\n",
    "    n_nodes = 100\n",
    "    \n",
    "if which_data == 'HCP_YA':\n",
    "    tr = 0.720\n",
    "    # tasks = ['tfMRI_EMOTION_LR', 'tfMRI_GAMBLING_LR', 'tfMRI_LANGUAGE_LR', 'tfMRI_RELATIONAL_LR', 'tfMRI_SOCIAL_LR', 'tfMRI_WM_LR']\n",
    "    tasks = ['tfMRI_EMOTION_LR', 'tfMRI_LANGUAGE_LR', 'tfMRI_RELATIONAL_LR', 'tfMRI_WM_LR']\n",
    "\n",
    "    # which_task = 'tfMRI_EMOTION_LR'\n",
    "    # which_task = 'tfMRI_GAMBLING_LR'\n",
    "    # which_task = 'tfMRI_LANGUAGE_LR'\n",
    "    # which_task = 'tfMRI_RELATIONAL_LR'\n",
    "    # which_task = 'tfMRI_SOCIAL_LR'\n",
    "    # which_task = 'tfMRI_WM_LR'\n",
    "    \n",
    "    # which_task = 'tfMRI_WM_RL'\n",
    "\n",
    "    # if which_task == 'tfMRI_EMOTION_LR':\n",
    "    #     contrasts_list = ['fear', 'neut']\n",
    "    #     task_performance_measures = ['Emotion_Task_Acc', 'Emotion_Task_Median_RT']\n",
    "    # elif which_task == 'tfMRI_GAMBLING_LR':\n",
    "    #     contrasts_list = ['win', 'loss']\n",
    "    #     task_performance_measures = ['Gambling_Task_Perc_Larger', 'Gambling_Task_Perc_Smaller']\n",
    "    # elif which_task == 'tfMRI_LANGUAGE_LR':\n",
    "    #     contrasts_list = ['math', 'story']\n",
    "    #     task_performance_measures = ['Language_Task_Acc', 'Language_Task_Median_RT']\n",
    "    # elif which_task == 'tfMRI_RELATIONAL_LR':\n",
    "    #     contrasts_list = ['match', 'relation']\n",
    "    #     task_performance_measures = ['Relational_Task_Acc', 'Relational_Task_Median_RT']\n",
    "    # elif which_task == 'tfMRI_SOCIAL_LR':\n",
    "    #     contrasts_list = ['mental', 'rnd']\n",
    "    #     task_performance_measures = ['Social_Task_Perc_Random', 'Social_Task_Perc_TOM']\n",
    "    # elif which_task == 'tfMRI_WM_LR' or which_task == 'tfMRI_WM_RL':\n",
    "    #     contrasts_list = ['0bk', '2bk']\n",
    "    #     # task_performance_measures = ['WM_Task_Acc', 'WM_Task_Median_RT', 'WM_Task_2bk_Acc', 'WM_Task_2bk_Median_RT', 'WM_Task_0bk_Acc', 'WM_Task_0bk_Median_RT']\n",
    "    #     task_performance_measures = ['WM_Task_Acc', 'WM_Task_Median_RT']\n",
    "    \n",
    "    cog_measures = [\n",
    "        'PMAT24_A_CR', # fluid intelligence\n",
    "        'VSPLOT_TC', # spatial orientation\n",
    "        'ListSort_Unadj', # working memory\n",
    "        'DDisc_AUC_40K', # delay discounting\n",
    "        'Flanker_Unadj', # executive function\n",
    "    ]\n",
    "elif which_data == 'HCP_D':\n",
    "    tr = 0.800\n",
    "    tasks = ['tfMRI_CARIT_PA', 'tfMRI_EMOTION_PA', 'tfMRI_GUESSING_PA']\n",
    "    # which_task = 'tfMRI_CARIT_PA'\n",
    "    # which_task = 'tfMRI_EMOTION_PA'\n",
    "    # which_task = 'tfMRI_GUESSING_PA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot states on brain surface\n",
    "annot_dir = '/home/lindenmp/research_projects/nctpy/data'\n",
    "lh_annot_file = os.path.join(annot_dir, 'schaefer_parc', 'fsaverage5', 'lh.Schaefer2018_{0}Parcels_7Networks_order.annot'.format(n_parcels))\n",
    "rh_annot_file = os.path.join(annot_dir, 'schaefer_parc', 'fsaverage5', 'rh.Schaefer2018_{0}Parcels_7Networks_order.annot'.format(n_parcels))\n",
    "fsaverage = datasets.fetch_surf_fsaverage(mesh=\"fsaverage5\")\n",
    "cmap = \"viridis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parc_centroids = pd.read_csv(os.path.join(indir, 'Schaefer2018_{0}Parcels_7Networks_order_FSLMNI152_1mm.Centroid_RAS.csv'.format(n_parcels)), index_col=1)\n",
    "parc_centroids.drop(columns=['ROI Label'], inplace=True)\n",
    "print(parc_centroids.head())\n",
    "\n",
    "distance_matrix = distance.pdist(\n",
    "    parc_centroids, \"euclidean\"\n",
    ")  # get euclidean distances between nodes\n",
    "distance_matrix = distance.squareform(distance_matrix)  # reshape to square matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjectids = np.loadtxt(os.path.join(indir, '{0}_{1}_subjids.txt'.format(which_data.replace('_', ''), atlas)), dtype=str)\n",
    "n_subs = len(subjectids)\n",
    "print(n_subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_axis = np.load(os.path.join(indir, 'schaefer{0}-7_sa-axis.npy'.format(n_parcels)))\n",
    "\n",
    "myelin = np.load(os.path.join(indir, '{0}_{1}_myelin.npy'.format(which_data.replace('_', ''), atlas)))\n",
    "myelin_mean = np.nanmean(myelin, axis=1)\n",
    "\n",
    "which_task = 'rfMRIREST1LR'\n",
    "rest_timescales = np.load(os.path.join(outdir, '{0}_{1}_{2}_timescales.npy'.format(which_data.replace('_', ''), atlas, which_task)))\n",
    "rest_timescales_mean = np.nanmean(rest_timescales, axis=0)\n",
    "\n",
    "print(sa_axis.shape, myelin_mean.shape, rest_timescales.shape, rest_timescales_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_timescales = dict()\n",
    "task_timescales_mean = dict()\n",
    "subject_filter = dict()\n",
    "task_timescales_mean_all = np.zeros(n_parcels)\n",
    "for which_task in tasks:\n",
    "    print(which_task)\n",
    "    task_timescales[which_task] = np.load(os.path.join(outdir, '{0}_{1}_{2}_timescales.npy'.format(which_data.replace('_', ''), atlas, which_task.replace('_', ''))))\n",
    "    try:\n",
    "        subject_filter[which_task] = np.load(os.path.join(outdir, '{0}_{1}_{2}_subjectfilter.npy'.format(which_data.replace('_', ''), atlas, which_task.replace('_', ''))))\n",
    "        print(subject_filter[which_task].sum())\n",
    "        task_timescales_mean[which_task] = np.nanmean(task_timescales[which_task][~subject_filter[which_task], :], axis=0)\n",
    "    except:\n",
    "        task_timescales_mean[which_task] = np.nanmean(task_timescales[which_task], axis=0)\n",
    "        \n",
    "    task_timescales_mean_all += task_timescales_mean[which_task]\n",
    "\n",
    "task_timescales_mean_all = np.divide(task_timescales_mean_all, len(tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = surface_plot(\n",
    "    data=sa_axis,\n",
    "    lh_annot_file=lh_annot_file,\n",
    "    rh_annot_file=rh_annot_file,\n",
    "    fsaverage=fsaverage,\n",
    "    order=\"lr\",\n",
    "    cmap=\"viridis\",\n",
    ")\n",
    "f.savefig(os.path.join(outdir, 'sa_axis.png'), dpi=600, bbox_inches=\"tight\", pad_inches=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(2.5, 2.5))\n",
    "reg_plot(rest_timescales_mean, myelin_mean, ylabel='Myelin (mean)', xlabel='ACF_lag (mean)', ax=ax, annotate='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = pd.DataFrame(index=np.arange(n_parcels))\n",
    "df_stats['rest'] = rest_timescales_mean\n",
    "df_stats['task'] = task_timescales_mean_all\n",
    "df_stats['SA'] = sa_axis\n",
    "df_stats = sm.add_constant(df_stats)\n",
    "results = smf.ols(formula='SA ~ rest * task', data=df_stats).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int_deflections = compute_deflections(rest_timescales_mean, task_timescales_mean_all, nuisance_regression=True)\n",
    "int_deflections = task_timescales_mean_all - rest_timescales_mean\n",
    "\n",
    "f, ax = plt.subplots(1, 3, figsize=(2*3, 1.75))\n",
    "reg_plot(sa_axis, rest_timescales_mean, xlabel='SA axis', ylabel='INTs (rest)', ax=ax[0], annotate='both', add_pval=False)\n",
    "reg_plot(sa_axis, task_timescales_mean_all, xlabel='SA axis', ylabel='INTs (task)', ax=ax[1], annotate='both', add_pval=False)\n",
    "reg_plot(sa_axis, int_deflections, xlabel='SA axis', ylabel='INTs (task-rest)', ax=ax[2], annotate='both', add_pval=False)\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()\n",
    "f.savefig(os.path.join(outdir, 'ints_deflections_sa_corr.svg'), dpi=600, bbox_inches=\"tight\", pad_inches=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = surface_plot(\n",
    "    data=rest_timescales_mean,\n",
    "    lh_annot_file=lh_annot_file,\n",
    "    rh_annot_file=rh_annot_file,\n",
    "    fsaverage=fsaverage,\n",
    "    order=\"lr\",\n",
    "    cmap=\"magma\",\n",
    ")\n",
    "f.savefig(os.path.join(outdir, 'ints_rest.png'), dpi=600, bbox_inches=\"tight\", pad_inches=0.01)\n",
    "\n",
    "f = surface_plot(\n",
    "    data=task_timescales_mean_all,\n",
    "    lh_annot_file=lh_annot_file,\n",
    "    rh_annot_file=rh_annot_file,\n",
    "    fsaverage=fsaverage,\n",
    "    order=\"lr\",\n",
    "    cmap=\"magma\",\n",
    ")\n",
    "f.savefig(os.path.join(outdir, 'ints_task.png'), dpi=600, bbox_inches=\"tight\", pad_inches=0.01)\n",
    "\n",
    "f = surface_plot(\n",
    "    data=int_deflections,\n",
    "    lh_annot_file=lh_annot_file,\n",
    "    rh_annot_file=rh_annot_file,\n",
    "    fsaverage=fsaverage,\n",
    "    order=\"lr\",\n",
    "    cmap=\"coolwarm\",\n",
    ")\n",
    "f.savefig(os.path.join(outdir, 'ints_deflections.png'), dpi=600, bbox_inches=\"tight\", pad_inches=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subject-level effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cell = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_task = tasks[-1]\n",
    "print(which_task)\n",
    "\n",
    "if which_task == 'tfMRI_EMOTION_LR':\n",
    "    task_performance_measures = ['Emotion_Task_Acc', 'Emotion_Task_Median_RT']\n",
    "elif which_task == 'tfMRI_GAMBLING_LR':\n",
    "    task_performance_measures = ['Gambling_Task_Perc_Larger', 'Gambling_Task_Perc_Smaller']\n",
    "elif which_task == 'tfMRI_LANGUAGE_LR':\n",
    "    task_performance_measures = ['Language_Task_Acc', 'Language_Task_Median_RT']\n",
    "elif which_task == 'tfMRI_RELATIONAL_LR':\n",
    "    task_performance_measures = ['Relational_Task_Acc', 'Relational_Task_Median_RT']\n",
    "elif which_task == 'tfMRI_SOCIAL_LR':\n",
    "    task_performance_measures = ['Social_Task_Perc_Random', 'Social_Task_Perc_TOM']\n",
    "elif which_task == 'tfMRI_WM_LR' or which_task == 'tfMRI_WM_RL':\n",
    "    task_performance_measures = ['WM_Task_Acc', 'WM_Task_Median_RT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_cell:\n",
    "    n_subjects = len(subjectids)\n",
    "    column_filter = task_performance_measures.copy()\n",
    "    column_filter.extend(cog_measures)\n",
    "    print(column_filter)\n",
    "\n",
    "    if which_data == 'HCP_YA':\n",
    "        df_behavior = pd.read_csv('/mnt/storage_ssd_raid/research_data/HCP_YA/unrestricted_lindenmp_9_26_2022_16_52_14.csv', index_col=0)\n",
    "        df_behavior = df_behavior.loc[subjectids.astype(int)]\n",
    "        df_behavior_restricted = pd.read_csv('/mnt/storage_ssd_raid/research_data/HCP_YA/RESTRICTED_lindenmp_9_26_2022_16_42_3.csv', index_col=0)\n",
    "        df_behavior_restricted = df_behavior_restricted.loc[subjectids.astype(int)]\n",
    "        df_behavior['age'] = df_behavior_restricted['Age_in_Yrs']\n",
    "        df_behavior['sex'] = df_behavior['Gender'] == 'M'\n",
    "        df_behavior['sex'] = df_behavior['sex'].astype(int)\n",
    "\n",
    "        # for y_var in column_filter:\n",
    "            # df_behavior[y_var].fillna(np.nanmean(df_behavior[y_var]), inplace=True)\n",
    "\n",
    "        y_var = column_filter[0]\n",
    "        print(y_var)\n",
    "        if 'Acc' in y_var:\n",
    "            y_var_label = 'accuracy'\n",
    "        elif 'RT' in y_var:\n",
    "            y_var_label = 'RT'\n",
    "        else:\n",
    "            y_var_label = y_var\n",
    "\n",
    "        if y_var == 'WM_Task_Acc':\n",
    "            y_var_filter = df_behavior[y_var] >= 50\n",
    "        else:\n",
    "            y_var_filter = ~np.isnan(df_behavior[y_var].values)\n",
    "            # y_var_filter = np.zeros(n_subjects).astype(bool)\n",
    "            # y_var_filter[:] = True\n",
    "        print(np.sum(y_var_filter))\n",
    "            \n",
    "        nuis_covs = np.concatenate((sp.stats.zscore(df_behavior['age']).values[:, np.newaxis],\n",
    "                                    df_behavior['sex'].values[:, np.newaxis],\n",
    "                                    np.ones((n_subjects, 1))\n",
    "                                    ), axis=1)\n",
    "\n",
    "        # nuis_covs = np.concatenate((sp.stats.rankdata(df_behavior['age'])[:, np.newaxis],\n",
    "        #                             df_behavior['sex'].values[:, np.newaxis],\n",
    "        #                             np.ones((n_subjects, 1))\n",
    "        #                             ), axis=1)\n",
    "\n",
    "    elif which_data == 'HCP_D':\n",
    "        df_behavior = pd.read_csv('/mnt/storage_ssd_raid/research_data/HCP_D/behavior/socdem01.txt', sep='\\t', header=0)\n",
    "        df_behavior = df_behavior.iloc[1:]\n",
    "        behavior_subjectids = list(df_behavior['src_subject_id'])\n",
    "        behavior_subjectids = ['sub-' + s + 'V1MR' for s in behavior_subjectids]\n",
    "        df_behavior.index = behavior_subjectids\n",
    "        df_behavior.index.name = 'subjectids'\n",
    "\n",
    "        y_var = 'interview_age'\n",
    "        # df_behavior = df_behavior.loc[:, ['WM_Task_Acc', 'WM_Task_Median_RT', 'WM_Task_2bk_Acc', 'WM_Task_2bk_Median_RT', 'WM_Task_0bk_Acc', 'WM_Task_0bk_Median_RT']]\n",
    "        df_behavior = df_behavior.loc[:, [y_var, 'sex']]\n",
    "        df_behavior[y_var] = df_behavior[y_var].astype(int)\n",
    "        df_behavior = df_behavior.loc[subjectids]\n",
    "        y_var_filter = np.zeros(n_subjects).astype(bool)\n",
    "        y_var_filter[:] = True\n",
    "\n",
    "    df_behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_cell:\n",
    "    rest_timescales_resid = nuis_reg(nuis_covs, rest_timescales)\n",
    "    task_timescales_resid = nuis_reg(nuis_covs, task_timescales[which_task])\n",
    "    # int_deflections = task_timescales - rest_timescales\n",
    "    # int_deflections = task_timescales_resid - rest_timescales_resid\n",
    "    # int_deflections_resid = nuis_reg(rest_timescales_resid, task_timescales_resid)\n",
    "\n",
    "    # brain_data = rest_timescales_resid\n",
    "    brain_data = task_timescales_resid\n",
    "    # brain_data = int_deflections_resid\n",
    "\n",
    "    # Fit models for each region, plot SA axis by beta value\n",
    "    corr_map = np.zeros(n_parcels)\n",
    "    p_map = np.zeros(n_parcels)\n",
    "    for i in np.arange(n_parcels):\n",
    "        stats = sp.stats.pearsonr(brain_data[y_var_filter, i], df_behavior[y_var][y_var_filter])\n",
    "        corr_map[i] = stats[0]\n",
    "        p_map[i] = stats[1]\n",
    "\n",
    "        # corr_map[i] = compute_r2(brain_data[y_var_filter, i], df_behavior[y_var][y_var_filter].values)\n",
    "\n",
    "    p_map = get_fdr_p(p_map)\n",
    "    corr_map_filtered = corr_map.copy()\n",
    "    # corr_map_filtered[p_map>0.05] = np.nan\n",
    "\n",
    "    f2 = surface_plot(\n",
    "        data=corr_map_filtered,\n",
    "        lh_annot_file=lh_annot_file,\n",
    "        rh_annot_file=rh_annot_file,\n",
    "        fsaverage=fsaverage,\n",
    "        order=\"lr\",\n",
    "        cmap=\"coolwarm\",\n",
    "        # title_str='corr(int_deflections, behavior)'\n",
    "    )\n",
    "    f2.savefig(os.path.join(outdir, 'corr(int_deflections,behavior)_brainplot.png'), dpi=600, bbox_inches=\"tight\", pad_inches=0.01)\n",
    "\n",
    "    # Get slope of the task-rest ~ SA axis per subject, regress slope on performance\n",
    "    f, ax = plt.subplots(1, 1, figsize=(1.1, 1.1))\n",
    "    reg_plot(sa_axis, corr_map, xlabel='SA axis', ylabel='corr(INTs, {0})'.format(y_var_label), ax=ax, annotate='spearman', add_pval=False)\n",
    "    plt.show()\n",
    "    f.savefig(os.path.join(outdir, 'int_deflections_task_performance_sa_corr_1.svg'), dpi=600, bbox_inches=\"tight\", pad_inches=0.01)\n",
    "\n",
    "    corr_map = np.zeros(n_subjects)\n",
    "    for i in np.arange(n_subjects):\n",
    "        corr_map[i] = sp.stats.pearsonr(sa_axis, brain_data[i, :])[0]\n",
    "    f, ax = plt.subplots(1, 1, figsize=(1.1, 1.1))\n",
    "    reg_plot(df_behavior[y_var].values[y_var_filter], corr_map[y_var_filter], ylabel='corr(SA axis, INTs)', xlabel=y_var_label, ax=ax, annotate='spearman', add_pval=False)\n",
    "    plt.show()\n",
    "    f.savefig(os.path.join(outdir, 'int_deflections_task_performance_sa_corr_2.svg'), dpi=600, bbox_inches=\"tight\", pad_inches=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pygam import LinearGAM, s, f\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "from tqdm import tqdm  # for progress bar\n",
    "\n",
    "\n",
    "def fit_smooth_gam(X, y, n_splines=20, lam_range=np.logspace(-3, 3, 25), \n",
    "                  term_types='auto', n_folds=5, plot=False, \n",
    "                  return_model=False, verbose=0, random_state=None,\n",
    "                  progress_bar=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Fit a smooth Generalized Additive Model (GAM) with automated smoothing parameter selection.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like or pd.DataFrame\n",
    "        Input features (n_samples, n_features)\n",
    "    y : array-like\n",
    "        Target values (n_samples,)\n",
    "    n_splines : int or list, optional\n",
    "        Number of splines to use (default: 20). If list, specify per feature.\n",
    "    lam_range : list or array-like, optional\n",
    "        Range of lambda values to search (default: [0.1, 1, 10, 100])\n",
    "    term_types : str or list, optional\n",
    "        Term types ('auto', 's' for spline, 'f' for factor, or list per feature)\n",
    "    n_folds : int, optional\n",
    "        Number of CV folds (default: 5)\n",
    "    plot : bool, optional\n",
    "        Whether to plot partial dependence (default: False)\n",
    "    return_model : bool, optional\n",
    "        Whether to return fitted model (default: False)\n",
    "    verbose : int, optional\n",
    "        Verbosity level (0-2)\n",
    "    random_state : int, optional\n",
    "        Random seed for reproducibility\n",
    "    progress_bar : bool, optional\n",
    "        Whether to show progress bar (default: True)\n",
    "    **kwargs : dict\n",
    "        Additional arguments for LinearGAM\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple or array:\n",
    "        If return_model=False: y_pred\n",
    "        If return_model=True: (y_pred, gam, best_params)\n",
    "            where best_params contains 'lam' and 'n_splines'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from pygam import LinearGAM, s, f\n",
    "    except ImportError:\n",
    "        raise ImportError(\"pygam is required. Install with: pip install pygam\")\n",
    "    \n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    # Handle 1D input\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(-1, 1)\n",
    "    \n",
    "    n_features = X.shape[1]\n",
    "    lam_range = np.asarray(lam_range)\n",
    "    \n",
    "    # Process n_splines parameter\n",
    "    if isinstance(n_splines, int):\n",
    "        n_splines = [n_splines] * n_features\n",
    "    elif len(n_splines) != n_features:\n",
    "        raise ValueError(\"n_splines must be int or list with length equal to n_features\")\n",
    "    \n",
    "    # Determine term types\n",
    "    if term_types == 'auto':\n",
    "        term_types = []\n",
    "        for i in range(n_features):\n",
    "            unique_vals = len(np.unique(X[:, i]))\n",
    "            if unique_vals < 10 or np.issubdtype(X.dtype, np.integer):\n",
    "                term_types.append('f')\n",
    "            else:\n",
    "                term_types.append('s')\n",
    "    elif isinstance(term_types, str):\n",
    "        term_types = [term_types] * n_features\n",
    "    \n",
    "    # Create terms with proper n_splines for spline terms\n",
    "    terms = []\n",
    "    for i, term_type in enumerate(term_types):\n",
    "        if term_type == 's':\n",
    "            terms.append(s(i, n_splines=n_splines[i]))\n",
    "        elif term_type == 'f':\n",
    "            terms.append(f(i))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown term type: {term_type}\")\n",
    "    \n",
    "    # Prepare parameter grid\n",
    "    if lam_range.ndim == 1:\n",
    "        param_grid = [lam_range] * n_features\n",
    "    \n",
    "    # Generate all lambda combinations\n",
    "    lam_combinations = list(product(*param_grid))\n",
    "    \n",
    "    # Cross-validation setup\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    cv_results = []\n",
    "    \n",
    "    # Grid search with progress bar\n",
    "    iter_combinations = tqdm(lam_combinations, desc=\"Grid search\") if progress_bar else lam_combinations\n",
    "    \n",
    "    for lam in iter_combinations:\n",
    "        fold_scores = []\n",
    "        \n",
    "        for train_idx, val_idx in kf.split(X):\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            gam = LinearGAM(*terms, lam=lam, **kwargs)\n",
    "            gam.fit(X_train, y_train)\n",
    "            score = gam.score(X_val, y_val)\n",
    "            fold_scores.append(score)\n",
    "        \n",
    "        mean_score = np.mean(fold_scores)\n",
    "        cv_results.append({'lam': lam, 'mean_score': mean_score})\n",
    "        \n",
    "        if verbose >= 2:\n",
    "            print(f\"Lambda: {lam} - Mean CV score: {mean_score:.4f}\")\n",
    "    \n",
    "    # Find best parameters\n",
    "    best_result = max(cv_results, key=lambda x: x['mean_score'])\n",
    "    best_lam = best_result['lam']\n",
    "    \n",
    "    if verbose >= 1:\n",
    "        print(f\"\\nBest lambda: {best_lam} with score: {best_result['mean_score']:.4f}\")\n",
    "        print(f\"Using n_splines: {n_splines}\")\n",
    "    \n",
    "    # Fit final model\n",
    "    final_gam = LinearGAM(*terms, lam=best_lam, **kwargs)\n",
    "    final_gam.fit(X, y)\n",
    "    y_pred = final_gam.predict(X)\n",
    "    \n",
    "    # Plotting\n",
    "    if plot:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(3, 3 * n_features))\n",
    "        \n",
    "        for i in range(n_features):\n",
    "            plt.subplot(n_features, 1, i+1)\n",
    "            XX = final_gam.generate_X_grid(term=i)\n",
    "            plt.scatter(X[:, i], y, alpha=0.2, label='Data')\n",
    "            plt.plot(XX[:, i], final_gam.partial_dependence(term=i, X=XX), \n",
    "                    'r', linewidth=2, label='GAM fit')\n",
    "            \n",
    "            title = f\"Feature {i}\"\n",
    "            if term_types[i] == 's':\n",
    "                title += f\" (Î»={best_lam[i]:.2f}, n_splines={n_splines[i]})\"\n",
    "            plt.title(title)\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    if return_model:\n",
    "        best_params = {'lam': best_lam, 'n_splines': n_splines}\n",
    "        return y_pred, final_gam, best_params\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_colors = get_my_colors()\n",
    "\n",
    "n_cols = 3\n",
    "f, ax = plt.subplots(1, n_cols, figsize=(2*n_cols, 2))\n",
    "plot_col = 0\n",
    "\n",
    "# version 1, deflections\n",
    "# int_deflections = task_timescales - rest_timescales\n",
    "# int_deflections_resid = np.zeros(int_deflections.shape)\n",
    "# for i in np.arange(n_subjects):\n",
    "#     int_deflections_resid[i, :] = nuis_reg(rest_timescales[i, :], int_deflections[i, :], use_sklearn=True)[:, 0]\n",
    "    \n",
    "# corr_map = np.zeros(n_parcels)\n",
    "# p_map = np.zeros(n_parcels)\n",
    "# for i in np.arange(n_parcels):\n",
    "#     stats = sp.stats.pearsonr(int_deflections[y_var_filter, i], df_behavior[y_var][y_var_filter])\n",
    "#     # stats = sp.stats.pearsonr(int_deflections_resid[y_var_filter, i], df_behavior[y_var][y_var_filter])\n",
    "#     corr_map[i] = stats[0]\n",
    "#     p_map[i] = stats[1]\n",
    "\n",
    "# reg_plot(sa_axis, corr_map, xlabel='SA axis', ylabel='corr(INTs_deflection, {0})'.format(y_var), ax=ax[plot_col], annotate='both', order=1, add_pval=False)  # 'coef({0}, {1})'.format(effect,y_var)\n",
    "\n",
    "# version 2, interaction\n",
    "# int_deflections = task_timescales_resid - rest_timescales_resid\n",
    "int_deflections = task_timescales[which_task] - rest_timescales\n",
    "corr_map_rest = np.zeros(n_parcels)\n",
    "corr_map_task = np.zeros(n_parcels)\n",
    "corr_map_delta = np.zeros(n_parcels)\n",
    "for i in np.arange(n_parcels):\n",
    "    stats = sp.stats.pearsonr(rest_timescales_resid[y_var_filter, i], df_behavior[y_var][y_var_filter])\n",
    "    corr_map_rest[i] = stats[0]\n",
    "    \n",
    "    stats = sp.stats.pearsonr(task_timescales_resid[y_var_filter, i], df_behavior[y_var][y_var_filter])\n",
    "    corr_map_task[i] = stats[0]\n",
    "    \n",
    "    stats = sp.stats.pearsonr(int_deflections[y_var_filter, i], df_behavior[y_var][y_var_filter])\n",
    "    corr_map_delta[i] = stats[0]\n",
    "\n",
    "df_stats = pd.DataFrame(index=np.arange(n_parcels))\n",
    "df_stats['rest'] = corr_map_rest\n",
    "df_stats['task'] = corr_map_task\n",
    "df_stats['SA'] = sa_axis\n",
    "df_stats = sm.add_constant(df_stats)\n",
    "results = smf.ols(formula='SA ~ rest : task', data=df_stats).fit()\n",
    "\n",
    "sns.regplot(x=sa_axis, y=corr_map_rest, ax=ax[plot_col], scatter=False, marker='.', scatter_kws={\"s\": 1}, label='rest', line_kws={'linestyle': '-'}, color=my_colors['north_sea_green'], order=1)\n",
    "sns.regplot(x=sa_axis, y=corr_map_task, ax=ax[plot_col], scatter=False, marker='.', scatter_kws={\"s\": 1}, label='task', line_kws={'linestyle': '--'}, color=my_colors['north_sea_green'], order=1)\n",
    "ax[plot_col].set_xlabel('SA axis')\n",
    "ax[plot_col].set_ylabel('corr(INTs, {0})'.format(y_var))\n",
    "ax[plot_col].set_title('t = {:.2f}, p = {:.2f}'.format(np.abs(results.tvalues['rest:task']), results.pvalues['rest:task']))\n",
    "ax[plot_col].legend()\n",
    "\n",
    "plot_col += 1\n",
    "\n",
    "q_val = 0.2\n",
    "lower_q = sa_axis < np.quantile(sa_axis, q=q_val)\n",
    "upper_q = sa_axis > np.quantile(sa_axis, q=1-q_val)\n",
    "\n",
    "rest_timescales_mean_bin = rest_timescales_resid[:, lower_q].mean(axis=1)\n",
    "task_timescales_mean_bin = task_timescales_resid[:, lower_q].mean(axis=1)\n",
    "y = df_behavior[y_var][y_var_filter]\n",
    "x = rest_timescales_mean_bin[y_var_filter]\n",
    "sns.regplot(x=x, y=y, ax=ax[plot_col], scatter=False, marker='.', scatter_kws={\"s\": 1}, label='rest', line_kws={'linestyle': '-'}, color='orange', order=1)\n",
    "r_rest = sp.stats.pearsonr(x, y)[0]\n",
    "x = task_timescales_mean_bin[y_var_filter]\n",
    "sns.regplot(x=x, y=y, ax=ax[plot_col], scatter=False, marker='.', scatter_kws={\"s\": 1}, label='task', line_kws={'linestyle': '--'}, color='orange', order=1)\n",
    "r_task = sp.stats.pearsonr(x, y)[0]\n",
    "\n",
    "ax[plot_col].set_xlabel('INTs')\n",
    "ax[plot_col].set_ylabel(y_var)\n",
    "ax[plot_col].set_title('Sensorimotor')\n",
    "# ax[plot_col].set_title('Sensorimotor\\nr_rest={:.2f}, r_task={:.2f}'.format(r_rest, r_task))\n",
    "# ax[plot_col].set_title('Sensorimotor (bottom {0}%)'.format(int(q_val*100)))\n",
    "ax[plot_col].legend()\n",
    "plot_col += 1\n",
    "\n",
    "rest_timescales_mean_bin = rest_timescales_resid[:, upper_q].mean(axis=1)\n",
    "task_timescales_mean_bin = task_timescales_resid[:, upper_q].mean(axis=1)\n",
    "x = rest_timescales_mean_bin[y_var_filter]\n",
    "sns.regplot(x=x, y=y, ax=ax[plot_col], scatter=False, marker='.', scatter_kws={\"s\": 1}, label='rest', line_kws={'linestyle': '-'}, color='purple', order=1)\n",
    "r_rest = sp.stats.pearsonr(x, y)[0]\n",
    "x = task_timescales_mean_bin[y_var_filter]\n",
    "sns.regplot(x=x, y=y, ax=ax[plot_col], scatter=False, marker='.', scatter_kws={\"s\": 1}, label='task', line_kws={'linestyle': '--'}, color='purple', order=1)\n",
    "r_task = sp.stats.pearsonr(x, y)[0]\n",
    "\n",
    "ax[plot_col].set_xlabel('INTs')\n",
    "ax[plot_col].set_ylabel(y_var)\n",
    "ax[plot_col].set_title('Association')\n",
    "# ax[plot_col].set_title('Association\\nr_rest={:.2f}, r_task={:.2f}'.format(r_rest, r_task))\n",
    "# ax[plot_col].set_title('Association (top {0}%)'.format(int(q_val*100)))\n",
    "ax[plot_col].legend()\n",
    "\n",
    "for this_ax in ax.reshape(-1):\n",
    "    sns.despine(right=True, top=True, ax=this_ax)\n",
    "\n",
    "# f.suptitle(which_task)\n",
    "f.tight_layout()\n",
    "plt.show()\n",
    "f.savefig(os.path.join(outdir, 'interaction_plot.svg'), dpi=600, bbox_inches=\"tight\", pad_inches=0.01)\n",
    "\n",
    "f2 = surface_plot(\n",
    "    data=corr_map_delta,\n",
    "    lh_annot_file=lh_annot_file,\n",
    "    rh_annot_file=rh_annot_file,\n",
    "    fsaverage=fsaverage,\n",
    "    order=\"lr\",\n",
    "    cmap=\"coolwarm\",\n",
    "    # title_str='corr(int_deflections, behavior)'\n",
    ")\n",
    "f2.savefig(os.path.join(outdir, 'interaction_plot_brain.png'), dpi=600, bbox_inches=\"tight\", pad_inches=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_data = np.zeros((len(cog_measures), 5))\n",
    "heatmap_data_pval = np.zeros((len(cog_measures), 5))\n",
    "\n",
    "for idx, y_var in enumerate(cog_measures):\n",
    "    y_var_filter = ~np.isnan(df_behavior[y_var].values)\n",
    "    print(np.sum(y_var_filter))\n",
    "    \n",
    "    corr_map_rest = np.zeros(n_parcels)\n",
    "    corr_map_task = np.zeros(n_parcels)\n",
    "    for i in np.arange(n_parcels):\n",
    "        stats = sp.stats.pearsonr(rest_timescales_resid[y_var_filter, i], df_behavior[y_var][y_var_filter])\n",
    "        corr_map_rest[i] = stats[0]\n",
    "        \n",
    "        stats = sp.stats.pearsonr(task_timescales_resid[y_var_filter, i], df_behavior[y_var][y_var_filter])\n",
    "        corr_map_task[i] = stats[0]\n",
    "\n",
    "    df_stats = pd.DataFrame(index=np.arange(n_parcels))\n",
    "    df_stats['rest'] = corr_map_rest\n",
    "    df_stats['task'] = corr_map_task\n",
    "    df_stats['SA'] = sa_axis\n",
    "    df_stats = sm.add_constant(df_stats)\n",
    "    results = smf.ols(formula='SA ~ rest : task', data=df_stats).fit()\n",
    "    \n",
    "    heatmap_data[idx, 0] = results.tvalues['rest:task']\n",
    "    heatmap_data_pval[idx, 0] = results.pvalues['rest:task']\n",
    "\n",
    "    y = df_behavior[y_var][y_var_filter]\n",
    "\n",
    "    rest_timescales_mean_bin = rest_timescales_resid[:, lower_q].mean(axis=1)\n",
    "    task_timescales_mean_bin = task_timescales_resid[:, lower_q].mean(axis=1)\n",
    "    heatmap_data[idx, 1] = sp.stats.pearsonr(rest_timescales_mean_bin[y_var_filter], y)[0]\n",
    "    heatmap_data[idx, 2] = sp.stats.pearsonr(task_timescales_mean_bin[y_var_filter], y)[0]\n",
    "\n",
    "    rest_timescales_mean_bin = rest_timescales_resid[:, upper_q].mean(axis=1)\n",
    "    task_timescales_mean_bin = task_timescales_resid[:, upper_q].mean(axis=1)\n",
    "    heatmap_data[idx, 3] = sp.stats.pearsonr(rest_timescales_mean_bin[y_var_filter], y)[0]\n",
    "    heatmap_data[idx, 4] = sp.stats.pearsonr(task_timescales_mean_bin[y_var_filter], y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(3, 1, figsize=(1, 2))\n",
    "# f, ax = plt.subplots(3, 1, figsize=(1.25, 2))\n",
    "heatmap_labels = [my_str.split('_')[0] for my_str in cog_measures]\n",
    "# heatmap_labels = [my_str.split('_')[0] for my_str in column_filter]\n",
    "# heatmap_labels[0] = 'accuracy'\n",
    "# heatmap_labels[1] = 'RT'\n",
    "print(heatmap_labels)\n",
    "\n",
    "markerline, stemlines, baseline = ax[0].stem(np.abs(heatmap_data[:, 0]))\n",
    "plt.setp(stemlines, color=my_colors['north_sea_green'])\n",
    "plt.setp(markerline, markersize=2, markeredgecolor=my_colors['north_sea_green'], markeredgewidth=0.5, markerfacecolor=my_colors['north_sea_green'])\n",
    "plt.setp(baseline, visible=False)\n",
    "ax[0].axhline(y=0, color='grey', linestyle=':')\n",
    "ax[0].set_ylabel('t-statistic')\n",
    "ax[0].set_xticklabels('')\n",
    "ax[0].set_xticks([])\n",
    "\n",
    "markerline, stemlines, baseline = ax[1].stem(np.arange(len(heatmap_labels))-0.1, heatmap_data[:, 1], linefmt='-', label='rest')\n",
    "plt.setp(stemlines, color='orange')\n",
    "plt.setp(markerline, markersize=2, markeredgecolor='orange', markeredgewidth=0.5, markerfacecolor='orange')\n",
    "plt.setp(baseline, visible=False)\n",
    "markerline, stemlines, baseline = ax[1].stem(np.arange(len(heatmap_labels))+0.1, heatmap_data[:, 2], linefmt='--', label='task')\n",
    "plt.setp(stemlines, color='orange')\n",
    "plt.setp(markerline, markersize=2, markeredgecolor='orange', markeredgewidth=0.5, markerfacecolor='orange')\n",
    "plt.setp(baseline, visible=False)\n",
    "ax[1].axhline(y=0, color='grey', linestyle=':')\n",
    "ax[1].set_ylabel('slope (r)')\n",
    "ax[1].set_xticklabels('')\n",
    "ax[1].set_xticks([])\n",
    "\n",
    "markerline, stemlines, baseline = ax[2].stem(np.arange(len(heatmap_labels))-0.1, heatmap_data[:, 3], linefmt='-', label='rest')\n",
    "plt.setp(stemlines, color='purple')\n",
    "plt.setp(markerline, markersize=2, markeredgecolor='purple', markeredgewidth=0.5, markerfacecolor='purple')\n",
    "plt.setp(baseline, visible=False)\n",
    "markerline, stemlines, baseline = ax[2].stem(np.arange(len(heatmap_labels))+0.1, heatmap_data[:, 4], linefmt='--', label='task')\n",
    "plt.setp(stemlines, color='purple')\n",
    "plt.setp(markerline, markersize=2, markeredgecolor='purple', markeredgewidth=0.5, markerfacecolor='purple')\n",
    "plt.setp(baseline, visible=False)\n",
    "ax[2].axhline(y=0, color='grey', linestyle=':')\n",
    "ax[2].set_ylabel('slope (r)')\n",
    "ax[2].set_xticks(np.arange(len(heatmap_labels)))\n",
    "ax[2].set_xticklabels(heatmap_labels, rotation=90, ha='center')\n",
    "\n",
    "# f.tight_layout()\n",
    "plt.show()\n",
    "f.savefig(os.path.join(outdir, 'interaction_stemplot.svg'), dpi=600, bbox_inches=\"tight\", pad_inches=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cell = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_cell:\n",
    "    n_rows = len(cog_measures)\n",
    "    n_cols = 3\n",
    "    f, ax = plt.subplots(n_rows, n_cols, figsize=(2.5*n_cols, 2.5*n_rows))\n",
    "    for plot_row, y_var in enumerate(cog_measures):\n",
    "        print(y_var)\n",
    "        effect_map, pvals = run_regression(df_behavior, y_var, rest_timescales, task_timescales)\n",
    "        plot_col = 0\n",
    "        # for effect in ['rest', 'task', 'rest:task']:\n",
    "        for effect in ['rest:task',]:\n",
    "            x = sa_axis[:, np.newaxis]\n",
    "            y = effect_map[effect].values[:, np.newaxis]\n",
    "            x_quad = np.hstack([x, x**2])\n",
    "            quad_model = LinearRegression()\n",
    "            quad_model.fit(x_quad, y)\n",
    "            y_pred_quad = quad_model.predict(x_quad)\n",
    "            r2_quad = r2_score(y, y_pred_quad)*100\n",
    "\n",
    "            reg_plot(x, y, xlabel='SA axis', ylabel='coef({0})'.format(effect), ax=ax[plot_row, plot_col], annotate='both', order=2, add_pval=False)  # 'coef({0}, {1})'.format(effect,y_var)\n",
    "            if plot_col == 0:\n",
    "                if y_var in task_performance_measures:\n",
    "                    ax[plot_row, plot_col].set_title('{:} (in scanner)\\nr2: {:.2f}%'.format(y_var, r2_quad))\n",
    "                else:\n",
    "                    ax[plot_row, plot_col].set_title('{:} (out of scanner)\\nr2: {:.2f}%'.format(y_var, r2_quad))\n",
    "            plot_col += 1\n",
    "\n",
    "            # if effect == 'rest:task':\n",
    "            #     plot_data = effect_map[effect].values\n",
    "            #     # p_map = pvals[effect].values\n",
    "            #     # p_map = get_fdr_p(pvals[effect])\n",
    "            #     # plot_data[p_map>0.05] = np.nan\n",
    "            #     f2 = surface_plot(\n",
    "            #         data=effect_map[effect].values,\n",
    "            #         lh_annot_file=lh_annot_file,\n",
    "            #         rh_annot_file=rh_annot_file,\n",
    "            #         fsaverage=fsaverage,\n",
    "            #         order=\"lr\",\n",
    "            #         cmap=\"coolwarm\",\n",
    "            #         # title_str='corr(int_deflections, behavior)'\n",
    "            #     )\n",
    "            #     f2.savefig(os.path.join(outdir, 'interaction_brainplot.png'), dpi=600, bbox_inches=\"tight\", pad_inches=0.01)\n",
    "        \n",
    "        q_val = 0.2\n",
    "        lower_q = sa_axis < np.quantile(sa_axis, q=q_val)\n",
    "        upper_q = sa_axis > np.quantile(sa_axis, q=1-q_val)\n",
    "\n",
    "        rest_timescales_mean_bin = nuis_reg(nuis_covs, rest_timescales)[lower_q, :].mean(axis=0)\n",
    "        task_timescales_mean_bin = nuis_reg(nuis_covs, task_timescales)[lower_q, :].mean(axis=0)\n",
    "        sns.regplot(y=df_behavior[y_var][y_var_filter], x=rest_timescales_mean_bin[y_var_filter], ax=ax[plot_row, plot_col], scatter=False, marker='.', scatter_kws={\"s\": 1}, label='rest', line_kws={'linestyle': '-'}, color='orange', order=1)\n",
    "        sns.regplot(y=df_behavior[y_var][y_var_filter], x=task_timescales_mean_bin[y_var_filter], ax=ax[plot_row, plot_col], scatter=False, marker='.', scatter_kws={\"s\": 1}, label='task', line_kws={'linestyle': '--'}, color='orange', order=1)\n",
    "        ax[plot_row, plot_col].set_xlabel('INTs')\n",
    "        ax[plot_row, plot_col].set_ylabel(y_var)\n",
    "        ax[plot_row, plot_col].set_title('Sensorimotor (bottom {0}%)'.format(int(q_val*100)))\n",
    "        ax[plot_row, plot_col].legend()\n",
    "        plot_col += 1\n",
    "        \n",
    "        rest_timescales_mean_bin = nuis_reg(nuis_covs, rest_timescales)[upper_q, :].mean(axis=0)\n",
    "        task_timescales_mean_bin = nuis_reg(nuis_covs, task_timescales)[upper_q, :].mean(axis=0)\n",
    "        sns.regplot(y=df_behavior[y_var][y_var_filter], x=rest_timescales_mean_bin[y_var_filter], ax=ax[plot_row, plot_col], scatter=False, marker='.', scatter_kws={\"s\": 1}, label='rest', line_kws={'linestyle': '-'}, color='purple', order=1)\n",
    "        sns.regplot(y=df_behavior[y_var][y_var_filter], x=task_timescales_mean_bin[y_var_filter], ax=ax[plot_row, plot_col], scatter=False, marker='.', scatter_kws={\"s\": 1}, label='task', line_kws={'linestyle': '--'}, color='purple', order=1)\n",
    "        ax[plot_row, plot_col].set_xlabel('INTs')\n",
    "        ax[plot_row, plot_col].set_ylabel(y_var)\n",
    "        ax[plot_row, plot_col].set_title('Association (top {0}%)'.format(int(q_val*100)))\n",
    "        ax[plot_row, plot_col].legend()\n",
    "\n",
    "    for this_ax in ax.reshape(-1):\n",
    "        sns.despine(right=True, top=True, ax=this_ax)\n",
    "\n",
    "    f.suptitle(which_task+'\\n\\n')\n",
    "    f.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantile effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cell = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_cell:\n",
    "    lower_q = sa_axis < np.quantile(sa_axis, q=0.2)\n",
    "    upper_q = sa_axis > np.quantile(sa_axis, q=0.8)\n",
    "    print(sa_axis[lower_q].mean(), sa_axis[upper_q].mean())\n",
    "\n",
    "    f, ax = plt.subplots(1, 2, figsize=(5, 2.75))\n",
    "\n",
    "    rest_timescales_mean_bin = np.nanmean(rest_timescales[lower_q, :], axis=0)\n",
    "    task_timescales_mean_bin = np.nanmean(task_timescales[lower_q, :], axis=0)\n",
    "\n",
    "    int_deflections = task_timescales_mean_bin - rest_timescales_mean_bin\n",
    "    reg_plot(df_behavior[y_var][y_var_filter], int_deflections[y_var_filter], xlabel=y_var, ylabel='INTs_def', ax=ax[0], annotate='both')\n",
    "    ax[0].set_title('Lower 20%\\n(i.e., S regions)')\n",
    "\n",
    "    rest_timescales_mean_bin = np.nanmean(rest_timescales[upper_q, :], axis=0)\n",
    "    task_timescales_mean_bin = np.nanmean(task_timescales[upper_q, :], axis=0)\n",
    "\n",
    "    int_deflections = task_timescales_mean_bin - rest_timescales_mean_bin\n",
    "    reg_plot(df_behavior[y_var][y_var_filter], int_deflections[y_var_filter], xlabel=y_var, ylabel='INTs_def', ax=ax[1], annotate='both')\n",
    "    ax[1].set_title('Upper 20%\\n(i.e., A regions)')\n",
    "\n",
    "    f.suptitle(which_task)\n",
    "    f.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_cell:\n",
    "    lower_q = df_behavior[y_var] < df_behavior[y_var].quantile(q=0.2)\n",
    "    upper_q = df_behavior[y_var] > df_behavior[y_var].quantile(q=0.8)\n",
    "    print(df_behavior[y_var][lower_q].mean(), df_behavior[y_var][upper_q].mean())\n",
    "\n",
    "    f, ax = plt.subplots(1, 2, figsize=(5, 2.5))\n",
    "\n",
    "    rest_timescales_mean_bin = np.nanmean(rest_timescales[:, lower_q.values], axis=1)\n",
    "    task_timescales_mean_bin = np.nanmean(task_timescales[:, lower_q.values], axis=1)\n",
    "\n",
    "    int_deflections = compute_deflections(rest_timescales_mean_bin, task_timescales_mean_bin, nuisance_regression=False)\n",
    "    reg_plot(sa_axis, int_deflections, xlabel='SA axis', ylabel='INTs_def', ax=ax[0], annotate='both')\n",
    "    ax[0].set_title('Lower 20%\\n({0})'.format(y_var))\n",
    "\n",
    "    rest_timescales_mean_bin = np.nanmean(rest_timescales[:, upper_q.values], axis=1)\n",
    "    task_timescales_mean_bin = np.nanmean(task_timescales[:, upper_q.values], axis=1)\n",
    "\n",
    "    int_deflections = compute_deflections(rest_timescales_mean_bin, task_timescales_mean_bin, nuisance_regression=False)\n",
    "    reg_plot(sa_axis, int_deflections, xlabel='SA axis', ylabel='INTs_def', ax=ax[1], annotate='both')\n",
    "    ax[1].set_title('Upper 20%\\n({0})'.format(y_var))\n",
    "\n",
    "    f.suptitle(which_task)\n",
    "    f.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nct_xr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
